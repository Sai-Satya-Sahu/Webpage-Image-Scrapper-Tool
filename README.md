# ğŸ‰ Welcome to My First Project

## ğŸŒ Webpage Image Scraper + ğŸ“„ PDF Converter

Welcome to my very first GitHub project!  
This tool is designed to **scrape all important document images from a webpage** â€” like scanned book pages or online document viewers â€” and **convert them into a PDF** automatically.

I hope you like my project and find it helpful! â¤ï¸

---

### âœ¨ Features

- âœ… **Smart Document Image Detection**  
  Automatically skips logos, ads, buttons, and other non-document images.

- ğŸ“œ **Deep Scroll & Lazy Load Support**  
  Scrolls the page fully and waits to load even lazy-loaded images.

- ğŸ” **Auto Retry on Timeout or Connection Issues**  
  Built-in retry mechanism ensures reliable downloads even on slow networks.

- ğŸ“‚ **Organized Output**  
  Downloads go into a folder named `downloaded_images/`.

- ğŸ§  **PDF Generator Tool**  
  Automatically combines the downloaded images into a clean, page-by-page PDF.

---

### ğŸ§  How Filtering Works

Only **important images** (like scanned pages or document content) are downloaded.  
We filter out common junk like:

- `logo`, `icon`, `ad`, `banner`, `favicon`, `share`, etc.
- Images smaller than `300x300` pixels

> âš™ï¸ **Want everything?**  
Just **remove or comment out** the `is_document_image()` function to disable filtering.

---

### ğŸ“¦ Project Structure

```bash
ğŸ“ downloaded_images/
â”œâ”€â”€ image_1.jpg
â”œâ”€â”€ image_2.jpg
â””â”€â”€ document.pdf        # Created using the PDF converter tool

ğŸ“„ doc_image_downloader.py   # Scrapes images from a webpage
ğŸ“„ images_to_pdf.py          # Converts images into a single PDF
```
### ğŸš€ Getting Started

1. Clone this repository:
```bash
   git clone https://github.com/Sai-Satya-Sahu/Webpage-Image-Scrapper-Tool.git
   cd Webpage-Image-Scrapper-Tool
```
2. Install dependencies:
```bash
   pip install -r requirements.txt
```
